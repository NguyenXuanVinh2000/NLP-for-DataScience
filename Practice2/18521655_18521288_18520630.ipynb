{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "18521655_18521288_18520630.ipynb",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chạy thử  https://github.com/neubig/anlp-code/blob/main/03-lm/nn-lm.py"
      ],
      "metadata": {
        "id": "iCTcMBiRg6a-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/neubig/anlp-code.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wak5JKIYe_Dn",
        "outputId": "de86fb02-4b37-4de6-e00c-5a85180f8cdc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'anlp-code'...\n",
            "remote: Enumerating objects: 52, done.\u001b[K\n",
            "remote: Counting objects: 100% (52/52), done.\u001b[K\n",
            "remote: Compressing objects: 100% (32/32), done.\u001b[K\n",
            "remote: Total 52 (delta 15), reused 47 (delta 11), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (52/52), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/anlp-code/03-lm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffMd_H23fhCq",
        "outputId": "a64646e0-1e22-4977-ba09-033b324fb07a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/anlp-code/03-lm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python nn-lm.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ip4qy7VfmOP",
        "outputId": "8acc74bf-611a-4eac-99d6-953ee03e90e8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n",
            "Starting training epoch 1 over 42068 sentences\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "4975it [00:15, 321.99it/s]--finished 5000 sentences (word/sec=6598.95)\n",
            "9990it [00:31, 313.89it/s]--finished 10000 sentences (word/sec=6618.67)\n",
            "14992it [00:47, 320.94it/s]--finished 15000 sentences (word/sec=6641.62)\n",
            "19975it [01:03, 305.92it/s]--finished 20000 sentences (word/sec=6658.31)\n",
            "24999it [01:18, 308.27it/s]--finished 25000 sentences (word/sec=6673.34)\n",
            "29977it [01:34, 318.09it/s]--finished 30000 sentences (word/sec=6693.58)\n",
            "34992it [01:50, 323.21it/s]--finished 35000 sentences (word/sec=6702.83)\n",
            "39988it [02:05, 323.45it/s]--finished 40000 sentences (word/sec=6707.75)\n",
            "42068it [02:12, 317.99it/s]\n",
            "iter 0: train loss/word=6.0560, ppl=426.6677 (word/sec=6708.28)\n",
            "epoch 0: dev loss/word=5.8711, ppl=354.6218 (word/sec=32905.46)\n",
            "i think the technology group of the international player is a consensus variety who asked a proved were the office in a former general counsel\n",
            "at least that in wichita\n",
            "the press surge to $ N a share a year ago\n",
            "reflecting with his lone broker as ago start-up in N when would lose the british governments that was a e. power is filled at exposed <unk> <unk> co. would have fallen it earthquake very much against light back change for example she says john miller said mr. guber noted over the next wednesday and kill\n",
            "bank funds are essential for their deposits and the operations shuttle gold problems\n",
            "Starting training epoch 2 over 42068 sentences\n",
            "4999it [00:15, 323.02it/s]--finished 5000 sentences (word/sec=6788.66)\n",
            "9971it [00:31, 321.46it/s]--finished 10000 sentences (word/sec=6751.67)\n",
            "14973it [00:46, 318.94it/s]--finished 15000 sentences (word/sec=6779.70)\n",
            "19993it [01:02, 317.73it/s]--finished 20000 sentences (word/sec=6778.68)\n",
            "24984it [01:18, 317.83it/s]--finished 25000 sentences (word/sec=6765.43)\n",
            "29978it [01:34, 322.45it/s]--finished 30000 sentences (word/sec=6752.63)\n",
            "34999it [01:49, 321.18it/s]--finished 35000 sentences (word/sec=6749.54)\n",
            "39992it [02:05, 317.53it/s]--finished 40000 sentences (word/sec=6738.21)\n",
            "42068it [02:11, 319.11it/s]\n",
            "iter 1: train loss/word=5.5757, ppl=263.9361 (word/sec=6732.25)\n",
            "epoch 1: dev loss/word=5.7680, ppl=319.9053 (word/sec=32930.89)\n",
            "<unk> financial unit and are the marketers were not courter of $ N million of assets\n",
            "mr. noriega a <unk> five years ago has had a improved track and marketing cars in english and recover and in <unk>\n",
            "at smith barney caught\n",
            "the canadian test and asking us by some tool and cd as much strength of <unk> inc. the gop indictment companies <unk>\n",
            "the gained N to $ N apiece\n",
            "Starting training epoch 3 over 42068 sentences\n",
            "4986it [00:15, 322.33it/s]--finished 5000 sentences (word/sec=6743.73)\n",
            "9973it [00:31, 311.00it/s]--finished 10000 sentences (word/sec=6717.97)\n",
            "14968it [00:47, 320.45it/s]--finished 15000 sentences (word/sec=6733.50)\n",
            "19976it [01:02, 318.07it/s]--finished 20000 sentences (word/sec=6726.03)\n",
            "24987it [01:18, 325.66it/s]--finished 25000 sentences (word/sec=6730.67)\n",
            "29985it [01:34, 315.54it/s]--finished 30000 sentences (word/sec=6726.77)\n",
            "34984it [01:49, 313.30it/s]--finished 35000 sentences (word/sec=6727.09)\n",
            "39974it [02:05, 319.76it/s]--finished 40000 sentences (word/sec=6726.68)\n",
            "42068it [02:11, 318.74it/s]\n",
            "iter 2: train loss/word=5.4029, ppl=222.0417 (word/sec=6724.50)\n",
            "epoch 2: dev loss/word=5.7460, ppl=312.9486 (word/sec=33144.21)\n",
            "to be possible software information and foreign power to <unk>\n",
            "a financial planner <unk> memory and stance to be feed through the merger of the world unit we believe mr. <unk> says that administrator\n",
            "the dow industrials holds more limited partnership a serious us\n",
            "an garrison proposal he gave it one approved the exchequer nigel lawson and the world business results include the league said that the <unk> dr. regular has had plans to lease discrimination on the world\n",
            "$ N million its operating campaign in lawyer 's new york state mayor bitter\n",
            "Starting training epoch 4 over 42068 sentences\n",
            "4972it [00:15, 319.70it/s]--finished 5000 sentences (word/sec=6638.08)\n",
            "9967it [00:31, 320.11it/s]--finished 10000 sentences (word/sec=6694.25)\n",
            "14989it [00:47, 319.20it/s]--finished 15000 sentences (word/sec=6700.54)\n",
            "19975it [01:02, 316.28it/s]--finished 20000 sentences (word/sec=6693.98)\n",
            "24973it [01:18, 320.69it/s]--finished 25000 sentences (word/sec=6692.83)\n",
            "29975it [01:34, 325.46it/s]--finished 30000 sentences (word/sec=6709.48)\n",
            "34978it [01:49, 315.79it/s]--finished 35000 sentences (word/sec=6712.77)\n",
            "39979it [02:05, 319.23it/s]--finished 40000 sentences (word/sec=6722.15)\n",
            "42068it [02:12, 318.64it/s]\n",
            "iter 3: train loss/word=5.2805, ppl=196.4755 (word/sec=6722.30)\n",
            "epoch 3: dev loss/word=5.7332, ppl=308.9456 (word/sec=33086.38)\n",
            "american medical co. the airlines traders has indicated it in a coupon for instance friday there has been under that mr. gorbachev was down as the report\n",
            "james a. commodities that can be to bad loans as president reagan of <unk> life insurance have a N N to N days last month already has been as smoothly in the case for the summer below N N\n",
            "revenue fell N N N of dreyfus products was held with her own plays\n",
            "the <unk> calif. louisville to N days N N to N billion yen from N trillion yen marks in compensation\n",
            "donaldson lufkin & jenrette securities oakland co. <unk> santa clara is not she encourage so lackluster had nearly significant board building a N N tax would will not be seen\n",
            "Starting training epoch 5 over 42068 sentences\n",
            "4977it [00:15, 320.44it/s]--finished 5000 sentences (word/sec=6691.63)\n",
            "9980it [00:31, 321.29it/s]--finished 10000 sentences (word/sec=6687.72)\n",
            "14978it [00:46, 323.47it/s]--finished 15000 sentences (word/sec=6711.56)\n",
            "19986it [01:02, 319.86it/s]--finished 20000 sentences (word/sec=6699.29)\n",
            "24991it [01:18, 322.26it/s]--finished 25000 sentences (word/sec=6723.44)\n",
            "29980it [01:33, 321.11it/s]--finished 30000 sentences (word/sec=6733.40)\n",
            "34980it [01:49, 322.27it/s]--finished 35000 sentences (word/sec=6734.57)\n",
            "39991it [02:05, 322.54it/s]--finished 40000 sentences (word/sec=6730.19)\n",
            "42068it [02:11, 319.26it/s]\n",
            "iter 4: train loss/word=5.1754, ppl=176.8713 (word/sec=6735.56)\n",
            "Traceback (most recent call last):\n",
            "  File \"nn-lm.py\", line 150, in <module>\n",
            "    optimizer.learning_rate /= 2\n",
            "AttributeError: 'Adam' object has no attribute 'learning_rate'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vEUvXV5yFAAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bài tập thực hành 2"
      ],
      "metadata": {
        "id": "-nIPWrk7hCE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnisMVpDHtAB",
        "outputId": "07a75b5e-9469-44d4-b9a8-d4ce2f8e5d03"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/NguyenXuanVinh2000/NLP-for-DataScience.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsOwWte0RvbV",
        "outputId": "a25934b3-7a9a-4767-cd51-d48264e0a6f1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NLP-for-DataScience'...\n",
            "remote: Enumerating objects: 21, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 21 (delta 0), reused 18 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (21/21), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "import os, sys"
      ],
      "metadata": {
        "id": "UzjVFiOKgK2P"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FNN_LM(nn.Module):\n",
        "  def __init__(self, nwords, emb_size, hid_size, num_hist):\n",
        "    super(FNN_LM, self).__init__()\n",
        "    self.embedding = nn.Embedding(nwords, emb_size)\n",
        "    self.fnn = nn.Sequential(\n",
        "      nn.Linear(num_hist*emb_size, hid_size),\n",
        "      nn.Tanh(),\n",
        "      nn.Linear(hid_size, nwords)\n",
        "    )\n",
        "\n",
        "  def forward(self, words):\n",
        "    emb = self.embedding(words)      # 3D Tensor of size [batch_size x num_hist x emb_size]\n",
        "    feat = emb.view(emb.size(0), -1) # 2D Tensor of size [batch_size x (num_hist*emb_size)]\n",
        "    logit = self.fnn(feat)           # 2D Tensor of size [batch_size x nwords]\n",
        "\n",
        "    return logit\n",
        "\n",
        "N = 2 # The length of the n-gram\n",
        "EMB_SIZE = 128 # The size of the embedding\n",
        "HID_SIZE = 128 # The size of the hidden layer\n",
        "\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "\n",
        "# Functions to read in the corpus\n",
        "# NOTE: We are using data from the Penn Treebank, which is already converted\n",
        "#       into an easy-to-use format with \"<unk>\" symbols. If we were using other\n",
        "#       data we would have to do pre-processing and consider how to choose\n",
        "#       unknown words, etc.\n",
        "w2i = {}\n",
        "S = w2i[\"<s>\"] = 0\n",
        "UNK = w2i[\"<unk>\"] = 1\n",
        "# def get_wid(w2i, x, add_vocab=True):\n",
        "#   if x not in w2i:\n",
        "#     if add_vocab:\n",
        "#       w2i[x] = len(w2i)\n",
        "#     else:\n",
        "#       return UNK\n",
        "#   return w2i[x]\n",
        "count = {}\n",
        "def get_wid(w2i, x, add_vocab=True):\n",
        "  if x not in w2i:\n",
        "    if add_vocab:\n",
        "      w2i[x] = len(w2i) \n",
        "      count[x] = 1\n",
        "    else:\n",
        "      return UNK\n",
        "  else:\n",
        "    count[x] = count[x] + 1\n",
        "  return w2i[x]\n",
        "\n",
        "def read_dataset(filename, add_vocab):\n",
        "  with open(filename, \"r\") as f:\n",
        "    for line in f:\n",
        "      yield [get_wid(w2i, x, add_vocab)  for x in line.strip().split(\" \")]\n",
        "\n",
        "# Read in the data\n",
        "train = list(read_dataset(\"/content/NLP-for-DataScience/Practice2/data/VLSP2013_raw_train.txt\", add_vocab=True))\n",
        "dev = list(read_dataset(\"/content/NLP-for-DataScience/Practice2/data/VLSP2013_raw_dev.txt\", add_vocab=False))\n",
        "test = list(read_dataset(\"/content/NLP-for-DataScience/Practice2/data/VLSP2013_raw_test.txt\", add_vocab=False))"
      ],
      "metadata": {
        "id": "v5b-fm1KLYZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Số câu văn huấn luyện trong tập train, dev và test là bao nhiêu?"
      ],
      "metadata": {
        "id": "cZ0C-VFXLbNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Số câu văn huấn luyện trong tập train: \", len(train))\n",
        "print(\"Số câu văn huấn luyện trong tập dev: \",len(dev))\n",
        "print(\"Số câu văn huấn luyện trong tập test: \",len(test))"
      ],
      "metadata": {
        "id": "7N2pqUlwgaUQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e97b7696-22ea-47d7-85be-aa6243089c09"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Số câu văn huấn luyện trong tập train:  23906\n",
            "Số câu văn huấn luyện trong tập dev:  2009\n",
            "Số câu văn huấn luyện trong tập test:  3481\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Kích thước của tập từ vựng là bao nhiêu?"
      ],
      "metadata": {
        "id": "3ritlJ1DLetj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i2w = {v: k for k, v in w2i.items()}\n",
        "nwords = len(w2i)\n",
        "print(nwords)\n",
        "for key in count:\n",
        "  if count[key] == 1:\n",
        "    # print(key)\n",
        "    w2i[key] = 1"
      ],
      "metadata": {
        "id": "P0NXkLgYgU5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3792b232-2bc1-488c-a98c-18449474ebbe"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24562\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize the model and the optimizer\n",
        "model = FNN_LM(nwords=nwords, emb_size=EMB_SIZE, hid_size=HID_SIZE, num_hist=N)\n",
        "if USE_CUDA:\n",
        "  model = model.cuda()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# convert a (nested) list of int into a pytorch Variable\n",
        "def convert_to_variable(words):\n",
        "  var = Variable(torch.LongTensor(words))\n",
        "  if USE_CUDA:\n",
        "    var = var.cuda()\n",
        "\n",
        "  return var\n",
        "\n",
        "# A function to calculate scores for one value\n",
        "def calc_score_of_histories(words):\n",
        "  # This will change from a list of histories, to a pytorch Variable whose data type is LongTensor\n",
        "  words_var = convert_to_variable(words)\n",
        "  logits = model(words_var)\n",
        "  return logits\n",
        "\n",
        "# Calculate the loss value for the entire sentence\n",
        "def calc_sent_loss(sent):\n",
        "  # The initial history is equal to end of sentence symbols\n",
        "  hist = [S] * N\n",
        "  # Step through the sentence, including the end of sentence token\n",
        "  all_histories = []\n",
        "  all_targets = []\n",
        "  for next_word in sent + [S]:\n",
        "    all_histories.append(list(hist))\n",
        "    all_targets.append(next_word)\n",
        "    hist = hist[1:] + [next_word]\n",
        "\n",
        "  logits = calc_score_of_histories(all_histories)\n",
        "  loss = nn.functional.cross_entropy(logits, convert_to_variable(all_targets), size_average=False)\n",
        "\n",
        "  return loss\n",
        "\n",
        "MAX_LEN = 100\n",
        "# Generate a sentence\n",
        "def generate_sent():\n",
        "  hist = [S] * N\n",
        "  sent = []\n",
        "  while True:\n",
        "    logits = calc_score_of_histories([hist])\n",
        "    prob = nn.functional.softmax(logits, 1)\n",
        "    multinom = prob.multinomial(1)\n",
        "    next_word = multinom.data.item()\n",
        "    if next_word == S or len(sent) == MAX_LEN:\n",
        "      break\n",
        "    sent.append(next_word)\n",
        "    hist = hist[1:] + [next_word]\n",
        "  return sent\n",
        "\n",
        "last_dev = 1e20\n",
        "best_dev = 1e20\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NuqGx_EFgb33"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Giá trị Perplexity (dựa trên per-word log likelihood) khi cho mô hình dự đoán trên tập train, dev và test là bao nhiêu?"
      ],
      "metadata": {
        "id": "CyiY73imZsYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(5):\n",
        "  # Perform training\n",
        "  random.shuffle(train)\n",
        "  # set the model to training mode\n",
        "  model.train()\n",
        "  train_words, train_loss = 0, 0.0\n",
        "  start = time.time()\n",
        "  print(f'Starting training epoch {epoch+1} over {len(train)} sentences')\n",
        "  for sent_id, sent in tqdm(enumerate(train)):\n",
        "    my_loss = calc_sent_loss(sent)\n",
        "    train_loss += my_loss.data\n",
        "    train_words += len(sent)\n",
        "    optimizer.zero_grad()\n",
        "    my_loss.backward()\n",
        "    optimizer.step()\n",
        "    if (sent_id+1) % 5000 == 0:\n",
        "      print(\"--finished %r sentences (word/sec=%.2f)\" % (sent_id+1, train_words/(time.time()-start)))\n",
        "  print(\"iter %r: train loss/word=%.4f, ppl=%.4f (word/sec=%.2f)\" % (epoch, train_loss/train_words, math.exp(train_loss/train_words), train_words/(time.time()-start)))\n",
        "  \n",
        "  # Evaluate on dev set\n",
        "  # set the model to evaluation mode\n",
        "  model.eval()\n",
        "  dev_words, dev_loss = 0, 0.0\n",
        "  start = time.time()\n",
        "  for sent_id, sent in enumerate(dev):\n",
        "    my_loss = calc_sent_loss(sent)\n",
        "    dev_loss += my_loss.data\n",
        "    dev_words += len(sent)\n",
        "\n",
        "  # Keep track of the development accuracy and reduce the learning rate if it got worse\n",
        "\n",
        "  if last_dev < dev_loss:\n",
        "    # optimizer.param_group['lr'] /= 2\n",
        "\n",
        "    for param_group in optimizer.param_groups: \n",
        "      param_group['lr'] /= 2 \n",
        "  last_dev = dev_loss\n",
        "  \n",
        "  # Keep track of the best development accuracy, and save the model only if it's the best one\n",
        "  if best_dev > dev_loss:\n",
        "    torch.save(model, \"model.pt\")\n",
        "    best_dev = dev_loss\n",
        "  \n",
        "  # Save the model\n",
        "  print(\"epoch %r: dev loss/word=%.4f, ppl=%.4f (word/sec=%.2f)\" % (epoch, dev_loss/dev_words, math.exp(dev_loss/dev_words), dev_words/(time.time()-start)))\n",
        "\n",
        "\n",
        "\n",
        "  test_words, test_loss = 0, 0.0\n",
        "  start = time.time()\n",
        "  test_ppl = {}\n",
        "  for sent_id, sent in enumerate(test):\n",
        "    my_loss = calc_sent_loss(sent)\n",
        "    test_loss += my_loss.data\n",
        "    test_words += len(sent)\n",
        "    test_ppl[sent_id] = math.exp(test_loss/test_words)\n",
        "  print(\"epoch %r: test loss/word=%.4f, ppl=%.4f (word/sec=%.2f)\" % (epoch, test_loss/test_words, math.exp(test_loss/test_words), test_words/(time.time()-start)))\n",
        "  # Generate a few sentences\n",
        "  for _ in range(5):\n",
        "    sent = generate_sent()\n",
        "    print(\" \".join([i2w[x] for x in sent]))"
      ],
      "metadata": {
        "id": "O3--lAUJgdym",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69dc8dd7-d9bd-4736-b94d-6244862f931d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training epoch 1 over 23906 sentences\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "5024it [00:29, 167.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--finished 5000 sentences (word/sec=3960.02)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "10024it [00:59, 167.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--finished 10000 sentences (word/sec=3961.64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "15020it [01:29, 164.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--finished 15000 sentences (word/sec=3979.31)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "20027it [01:59, 167.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--finished 20000 sentences (word/sec=3992.95)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "23906it [02:23, 167.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter 0: train loss/word=6.8900, ppl=982.3821 (word/sec=3992.28)\n",
            "epoch 0: dev loss/word=7.0603, ppl=1164.8111 (word/sec=24359.37)\n",
            "epoch 0: test loss/word=7.1529, ppl=1277.8467 (word/sec=24246.07)\n",
            "Một người biến .\n",
            "3 : Lao_động của ông ngửa .\n",
            "xâm_nhập Việt_Nam .\n",
            "Thực_tiễn tôi : \" sư_phạm gia_tăng , không mang lại cuộc hành_trình hàng_hải phục_vụ giảm nhanh thúng bó Bệnh_viện được Quốc_hội để giảm nhiều trẻ , các hoạt_động của Fred .\n",
            "nổi_bật là gai khu_vực , ngồi ba năm nhau và tạo nên mang nuôi bò thứ nhất , tôi chỉ bao_giờ không để bốn người khác hết để mất xe chở gò lá chiến_tranh , cô gái được nhận thấy được điều_chỉnh chính_sách đột_phá , trong đó có một lãnh_đạo đất_nước .\n",
            "Starting training epoch 2 over 23906 sentences\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5033it [00:30, 170.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--finished 5000 sentences (word/sec=3974.26)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "10030it [01:00, 169.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--finished 10000 sentences (word/sec=4009.25)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "15027it [01:29, 167.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--finished 15000 sentences (word/sec=4015.67)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "20033it [01:59, 169.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--finished 20000 sentences (word/sec=3999.18)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "23906it [02:23, 166.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter 1: train loss/word=6.2764, ppl=531.8589 (word/sec=3989.05)\n",
            "epoch 1: dev loss/word=7.0962, ppl=1207.3335 (word/sec=24790.90)\n",
            "epoch 1: test loss/word=7.2029, ppl=1343.3348 (word/sec=23769.15)\n",
            "Sự hình_thành môi_trường vì phát_triển động_vật ... \" trong tình_trạng em chỉ là Tiếp_tục vào các chú chim tình_cờ 200 - ông một con dạy nghề Trung sớm có chiều 16 lớp 1 . Đổi_mới , anh Lộc , con đường hải_lí biết cơ_hội vượt cam_kết vì quan_hệ lên rừng không_thể chịu phải bỏ bệnh nhưng rất khó xuất_khẩu , cao nên điều đó là sự tăng của Ấn_Độ ( dường_như ) bên ngoài đều được gọi_là đâu .\n",
            "Công_trình phẩm_chất ( Chương Hùng giúp gia_đình có được chứng_minh của các băng khác đã vay đến xã_hội càng nguy_hiểm \" , phấn_đấu lại sự ngoại_tệ của toán hiện_nay đã khắc khi được Hội_nghị , các hình_phạt , ông nguỵ_trang - cô_dâu ) , tiếp_cận sâu trong việc gia_nhập chất_lượng giáo_dục và phát_triển xã_hội của các chị_em , các nước vẫn đã chứ thể_hiện chất_lượng .\n",
            "Tất_nhiên vì Đinh_Nhiêu phân_phối , cấp phê_duyệt .\n",
            "1 cả 5 xin tại_sao liên_tục quay lại tạm_bợ đến tội_phạm_hoá ngày ... ngành , trong vòng cũng từ Cát_Bà bộ bờ kênh Kỳ 12 : Hôm_nay ) .\n",
            "đổi dự_thảo nhà .\n",
            "Starting training epoch 3 over 23906 sentences\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5026it [00:30, 167.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--finished 5000 sentences (word/sec=4005.90)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "10017it [01:00, 166.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--finished 10000 sentences (word/sec=4002.33)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "15018it [01:30, 165.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--finished 15000 sentences (word/sec=3981.72)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "20022it [02:00, 166.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--finished 20000 sentences (word/sec=3981.00)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "23906it [02:23, 166.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter 2: train loss/word=5.8699, ppl=354.2152 (word/sec=3969.89)\n",
            "epoch 2: dev loss/word=7.0369, ppl=1137.9069 (word/sec=23921.10)\n",
            "epoch 2: test loss/word=7.1436, ppl=1265.9608 (word/sec=24196.30)\n",
            "Một cô gái không_thể ăn nổi Đội thẳng xuống dưới địa_đạo thực_phẩm .\n",
            "Và \" , .\n",
            "cân\n",
            "Các đòn đối_xử .\n",
            "nhân_vật là có ai luyện đậm trên đất bao_gồm 10 đợt tấn_công tất_cả những Vĩnh_Hưng như_vậy .\n",
            "Starting training epoch 4 over 23906 sentences\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5023it [00:30, 166.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--finished 5000 sentences (word/sec=4036.21)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "10021it [01:00, 165.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--finished 10000 sentences (word/sec=4005.61)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "15031it [01:30, 166.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--finished 15000 sentences (word/sec=3986.93)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "20032it [02:00, 167.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--finished 20000 sentences (word/sec=3981.51)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "23906it [02:23, 166.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter 3: train loss/word=5.7592, ppl=317.0913 (word/sec=3981.29)\n",
            "epoch 3: dev loss/word=7.0716, ppl=1178.0707 (word/sec=24827.51)\n",
            "epoch 3: test loss/word=7.1745, ppl=1305.6454 (word/sec=24361.67)\n",
            "Anh Đây là một pha 21 , tôi tìm đến các người_dân ở ngoài sân trẻ , chúng_tôi cụ_thể là : thám_tử cứ chửi dời biển ;\n",
            "Việt_Nam hiện_nay .\n",
            "Nhìn mãi rất ngạc_nhiên vì nghe tài_xế đặt bị cung_cấp bởi pha xương ...\n",
            "Đoàn suốt ông Mỗi loại khiếu_nại , tố_cáo trước Mỹ nói về bảo_vệ bầy , cộng lại bằng địa_chỉ không để đánh_bắt cao .\n",
            "Một tấm ảnh trong Nam .\n",
            "Starting training epoch 5 over 23906 sentences\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5017it [00:30, 166.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--finished 5000 sentences (word/sec=3966.24)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "10017it [00:59, 165.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--finished 10000 sentences (word/sec=3970.89)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "15023it [01:30, 163.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--finished 15000 sentences (word/sec=3976.50)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "20033it [02:00, 167.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--finished 20000 sentences (word/sec=3975.89)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "23906it [02:23, 166.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter 4: train loss/word=5.5619, ppl=260.3146 (word/sec=3975.80)\n",
            "epoch 4: dev loss/word=7.0642, ppl=1169.3901 (word/sec=24640.56)\n",
            "epoch 4: test loss/word=7.1729, ppl=1303.6820 (word/sec=23921.02)\n",
            "Anh chỉ Pháp rộng vẫn 20 , mọi hành_vi có phần may_mắn tai_nạn thật với gia_đình anh còn 5 . 100 mìn cho người chiến_thắng .\n",
            "- Nhóm quần_hệ rừng chữ nhà_nước và .\n",
            "Vấn_đề mới chỉ là thế cũng không tay cả nhau .\n",
            "Nhiều tháng để con một cuộc chủ cá Hải không sống .\n",
            "Đồng_chí sẽ được đây là trong thực_tiễn .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_ppl_show = dict(sorted(test_ppl.items(), key=lambda item: item[1]))\n",
        "test_ppl_show = list(test_ppl_show.keys())\n",
        "test_ppl_show[-5:]"
      ],
      "metadata": {
        "id": "wK5OlYC4ghGW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e64b63b-c038-40d0-8eb5-d7fcec62a12e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[14, 10, 11, 13, 12]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4. Ghi nhận lại 5 câu văn mà mô hình ngôn ngữ cho giá trị per-word log-likelihood cao nhất trên tập test. Phân tích nhanh kết quả thu được."
      ],
      "metadata": {
        "id": "CavL4iYoZx_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_5_top = test_ppl_show[-5:]\n",
        "for i in show_5_top:\n",
        "  print(\" \".join([i2w[x] for x in test[i]]))"
      ],
      "metadata": {
        "id": "5-F_poQNgkE7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb37e6f6-c44a-439d-e0f6-5b146b01ec16"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Theo thông_báo của công_ty gửi về , Thanh mất ngày 19 - 8 - 2005 , khi đang làm_việc trên tàu đánh_cá ở vùng_biển gần Argentina nhưng mãi đến hơn tháng sau cả nhà mới biết tin .\n",
            "Trên tay chị , bé <unk> cũng khóc <unk> đòi bú nghe não ruột , nát gan .\n",
            "\" Một tuần nay , khi biết tin chồng chết biển , o ( cô ) Minh có thiết ăn_uống gì đâu , làm_gì mà còn sữa nuôi con_bé \" - bà hàng_xóm tên Ngân sụt_sùi lau ngang dòng lệ .\n",
            "Một buổi chiều cuối tháng chín , chị Minh như sụp_đổ khi nhận được hung tin chồng mình là <unk> ( 28 tuổi ) đã chết khi đi đánh_cá trên bờ biển Nam_Mỹ ( anh Thanh là thuyền_viên VN mới nhất tử_nạn ) .\n",
            "Ngoài_trời mưa <unk> , cảnh_vật quanh nhà vắng_vẻ , <unk> càng làm tăng thêm nỗi buồn người <unk> .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5. Ghi nhận lại 5 câu văn mà mô hình ngôn ngữ cho giá trị per-word log-likelihood thấp nhất trên tập test. Phân tích nhanh kết quả thu được."
      ],
      "metadata": {
        "id": "5u8usEMWZ35e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_5_bot = test_ppl_show[0:5]\n",
        "for i in show_5_bot:\n",
        "  print(\" \".join([i2w[x] for x in test[i]]))"
      ],
      "metadata": {
        "id": "-RVAwAWOglD5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "811b27a7-ca20-43bc-f8b7-ff974b042ec5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chị Minh ôm đứa con_gái mới hơn hai tháng rưỡi tuổi nấc lên từng tiếng thảm_thiết khi kể lại cho chúng_tôi nghe về cái chết của chồng .\n",
            "Bọn trẻ cũng nhìn các ảnh này quen rồi \" .\n",
            "Năm tháng sau anh mất , bọn chúng vẫn còn quá nhỏ , chỉ biết mặt bố qua mấy tấm ảnh này \" .\n",
            "\" Mỗi lần nhìn lại những ảnh này , tôi vẫn không_thể tin anh đã chết .\n",
            "Chỉ có năm tấm ảnh do những đồng_nghiệp , chủ tàu chụp : cảnh mọi người đang bó cái xác <unk> của anh , cảnh <unk> , cảnh đưa thi_hài anh vào lò thiêu ...\n"
          ]
        }
      ]
    }
  ]
}